# Notes: MIT 6.00.2x: Experimental Data


Trying out the [Feynman technique](https://mattyford.com/blog/2014/1/23/the-feynman-technique-model).


## Curve fitting

### What is it?

A technique to find a line (or whatever) that is the closest to all the points in a graph.

### Why is it important?

We do this to find out if there is a relationship between 2 things, independent x and dependent y.


Data is never perfect match for theory due to many factors like measuring error, so we use curve fitting to fit an independent variable (does not depend on another) to an estimated value of a dependent variable (depends on another).


We want to find a line such that some function of the sum of distances from the line to the measured points is minimized. (minimizing y)


This is commonly done through a function called Least Squares.

## Least Squares

### What is it?

A commonly used function that determines if our line fits the points. The smaller its value, the closer our line fits the points.

### How does it work?

Pseudo-code something like...

```
function least_squares(observerd, predicted):
  value = 0
  for x in range(len(observed)):
    value += (observed[i] - predicted[i]) ** 2
  return value
```

Squaring this value is cool because it always makes the value positive (doesn't care about sign) and is differentiable (will try to explain later)


## Linear regression

### What is it?

A technique to find the coefficients of a polynomial.

```
This is a polynomial...

3x^2 + 2x + 1

...and the coefficients are 3 and 2.
```

For an example of a linear regression problem:


Given `y = ax + b`, find values of a and b so that when you plug them in, they return the smallest Least Squares function value.


## pylab.polyfit or numpy.polyfit

Finds coefficients that minimize the squared error. It's doing linear regression!


args - observed x values, observed y values, n: degree (refresher: highest power in the whole expression) of polynomial


returns tuple of length: len(n) + 1


HINT: In the lessons, this is what is described as the `model`

e.g. n = 1 is a line, n = 2 is a parabola

## pylab.polyval

Basically applies coefficients to a polynomial.


args - coefficients and polynomial


## Average mean square error


The average of mean square errors. Use this function to compare the goodness of fit of different models for the same data. e.g. trying a model of degree 1 and degree 2. The model that returns lowest average mean square error is a better fit.


It's not so useful in getting the absolute goodness of fit though.

### What do you mean by absolute goodness of fit?

e.g. Is 1235 good? Is 500 good? Idk. It's just relative.


## Coefficient of determination or R squared

This is used to get the absolute goodness of fit. 1 - perfect score. Explains all variability of the original values. 0 - no relationship between values predicted by model and the actual data.

```
def rSquare(measured, estimated):
    """measured: one dimensional array of measured values
       estimate: one dimensional array of predicted values"""
    SEE = ((estimated - measured)**2).sum()
    mMean = measured.sum()/float(len(measured))
    MV = ((mMean - measured)**2).sum()
    return 1 - SEE/MV
```

## Overfitting

Sometimes when you do linear regression with a high degree, it results in a very high R^2, which is a good thing right?


Wrong. Just because R^2 increases doesn't mean we should do it. This is a training error and it happens because the model is trying to fit itself to the training data as well as it can.


How I understand it is it's kind of like you're fitting the line to the imperfections of the data, not the actual pattern. (update: Prof. Guttag says we're just fitting the noise)


We want our model to work well on other data (which has its own errors, think random errors) generated by the same process.


For an illustrative example, Prof. Guttag made a function generate a parabola with random errors. He then ran polyfit with a degree of 16 vs a polyfit with a degree of 2. We know 2nd degree polynomials are in fact parabolas. The degree 16 gave a higher R^2 though.


How to overcome? Here's the answer...

## Cross validation

Generate 2 sets of data with same parabola function (different because of calls to random).


Use polyfit to generate model for dataset 1 and test for R^2 on dataset 2 and vice versa.


Basically, use the coefficients we got while using linear regression on one model on the observed values of another model.


Once we do this, the degree 16 fit becomes shitty.


## Increasing complexity

When you increase degree of polynomial it always becomes a better fit to the training data.


Why? Because you can just set coefficient to zero (makes more values possible)


e.g. if its actually a line y = ax + b and you use something like y = zx^2 + ax + b the algorithm could just produce z = 0 to make it conform to a function of a line.


Higher degree means higher chance of fitting noise


## Overfitting

When a model is trained to fit training data too closely, this is called overfitting.


Overly complex model leads to overfitting.


Insufficiently complex model leads to probably crappy R^2.


"Everything should be made as simple as possible, but not simpler." - Albert Einstein


## Leave-one-out cross validation

Just remove one data point and test the model on the remaining data. Do that one time per point in dataset. Average the results. `results` in this context means R^2 per run.


This would take way too long though if you had millions of data.

## k-fold cross validation

Similar to leave one out.

Partition the data into k equal parts. Assume k = 4.
Pick one part, leave that one out. Train on the other 3 and test on the part that was left out. Average the results.

## repeated random sampling

Leave out a random fraction of the data set. Usually as little as 20%.


Randomly partition into 2 sets: training and test.


Do this n times (maybe around 100) and then average the results.

