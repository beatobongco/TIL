* Backprop - not the first to create it - got a paper published in Nature 1986
* People were impressed by features learned (trained on family tree / prediction?)
* psychology view - concept is just big bundle of features
* AI - concept is how it relates to other concepts (graphs, semantic nets)
* What own discoveries are you most excited about? 
  * Along with T. Sejnowski (from learning how to learn?) - **Boltzmann machines
  * Simple learning algorithm for big NNs that learns hidden representations
  * Each synapse only needs to know about the 2 neurons it was connected to
  * Sleep and wake phases
  * one layer of features -> treat as new data -> repeat 10x
* A ReLU was almost exactly equivalent to a whole stack of logistic units (!!!)
* Initialize NN (I think RNN) with an identity matrix + ReLUs (?) 300 hidden layers
* Recirculation algorithm -> check it out
* Capsules? Hinton believes strongly in this concept though a lot don't. Features can "vote" to agree with each other.
* Unsupervised learning may be the next big thing (generative adversarial nets) 
* Advice: read a little bit of the literature but don't read too much! (OOOH) 
Be contrarian, keep at it even if people don't believe.
* Either your intuitions are good or they're not. If they're good follow them and be successful, 
if they're not good doesn't matter what you do.
* Ng: Don't just read papers, replicate results.
* Never stop programming
* Read enough to develop your own intuitions then trust them. Don't care about others shooting down your ideas.
* Adapt to your advisor's beliefs for largest knowledge gain. Maybe can ask Marte which he thinks are the most 
fundamental skills vs what I'd rather learn. He IS the mentor.
* Hinton believes: Thoughts are big vectors that cause other big vectors 
