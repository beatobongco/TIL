Video: https://www.youtube.com/watch?v=HcStlHGpjN8

* Jeff Dean leads the Google Brain project.
* [Google Brain Residency](https://research.google.com/teams/brain/residency/) is like spending a year in a 
Master's or PhD program in deep learning.  Residents are expected to read papers, work on research projects, 
and encouraged to publish in top-tier venues. 
* Nowadays, because of great compute capability, NN are a great solution to an awful lot of problems.
* TF is second-gen! First gen is not OSS.
* TF seems to scale linearly with more GPU's! 
![image](https://user-images.githubusercontent.com/3739702/29748209-02152a06-8b44-11e7-8d83-914206e83879.png)
* TF supports a lot of languages but Python support is best, yeah! 
* Same model, different data, solve different problems. Given image, predict interesting pixels

![image](https://user-images.githubusercontent.com/3739702/29748254-030c4ce0-8b45-11e7-9f89-83719335a153.png)

![image](https://user-images.githubusercontent.com/3739702/29748256-147fda5a-8b45-11e7-967f-a2ebc195ddc6.png)

* Cells can now be virtually stained by neural nets!
* > 10% of mobile inbox replies are done via Smart Reply. Pipeline is actually small feedforward NN that will predict
if can smart reply. If yes, deep neural net will return top 3 plausible replies.
* Parameter servers - share parameters (Google published paper 2012)
* Learning to learn - research at Google, DL network that can learn NN architecture and optimizers

![image](https://user-images.githubusercontent.com/3739702/30250569-19c096be-9683-11e7-8e75-02226a9fefd5.png)

* NN is future

![image](https://user-images.githubusercontent.com/3739702/30250536-8638c8a8-9682-11e7-88fd-b3aabded6a97.png)


